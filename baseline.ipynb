{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical, Sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM, CuDNNLSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.utils import plot_model\n",
    "from IPython.display import SVG, display\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import keras.backend as K\n",
    "from sys import getsizeof\n",
    "from collections import Counter\n",
    "from numpy import argmax\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "matplotlib.rcParams['text.usetex'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference(dataset, interval=1):\n",
    "    \"\"\"\n",
    "    Calculates the difference between a time-series and a lagged version of it\n",
    "    \"\"\"\n",
    "    diff = list()\n",
    "    for i in range(interval, len(dataset)):\n",
    "        value = dataset[i] - dataset[i - interval]\n",
    "        diff.append(value)\n",
    "    return diff\n",
    "\n",
    "def create_windowed_dataset(data, look_back):\n",
    "    \"\"\"\n",
    "    Create the dataset by grouping windows of memory accesses together (using the look_back parameter)\n",
    "\n",
    "    data: it should be a list of integers\n",
    "    \n",
    "    QUESTION: very similar to create minibatches of data \n",
    "    \"\"\"\n",
    "    sequences = list()\n",
    "    for i in range(look_back, len(data)):\n",
    "        sequence = data[i-look_back:i+1]\n",
    "        sequences.append(sequence)\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute differences and encode the words\n",
    "vocabulary_mimimum_word_frequency = 10 \n",
    "dummy_word = \"0xffffffff\" \n",
    "\n",
    "def dataset_creator(app_name, use_manual_encoding):\n",
    "    '''\n",
    "    1. \n",
    "    '''\n",
    "    # read and build a dataframe based on input memory access files\n",
    "    dataframe = pd.read_csv(app_name, sep=' ')\n",
    "    \n",
    "    # set each of three columns to 'instruction', 'type', and 'address'\n",
    "    dataframe.columns = ['instruction', 'type', 'address']\n",
    "    \n",
    "    # we only need to keep addresses of type 'R'\n",
    "    dataframe = dataframe[dataframe['type'] == 'R']\n",
    "    \n",
    "    # we only keep the column 'address' by removing 'instructions' and 'type'\n",
    "    dataframe = dataframe['address']\n",
    "    \n",
    "    # build a new tokenizer from keras.preprocessing.text.Tokenizer\n",
    "    tokenizer = Tokenizer()\n",
    "    \n",
    "    # fit the dataframe by calling fit_on_texts so that we can assign a uniquely\n",
    "    # identified number to each unique address\n",
    "    tokenizer.fit_on_texts(list(dataframe))\n",
    "    \n",
    "    # As we fit_on_texts in the last step, we would fetch the corresponding \n",
    "    # identification number sequence from the input dataframe \n",
    "    encoded_raw = tokenizer.texts_to_sequences([' '.join(list(dataframe))])[0]\n",
    "    \n",
    "    # vocab_size_raw is the number of unique addresses + 1\n",
    "    vocab_size_raw = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    # declare encoded_final and final_vocab_size for the final output use \n",
    "    encoded_final = []\n",
    "    final_vocab_size = 0\n",
    "\n",
    "    if use_manual_encoding:\n",
    "        # calculate the difference and set the lagged interval to 1 in this case \n",
    "        \n",
    "        # QUESTION: if we assign a unique number to each unique address (randomly), we lose the locality \n",
    "        # information and hence the difference only keeps track of the difference between the previous address\n",
    "        # and the current address \n",
    "        encoded_raw_diff = difference(encoded_raw, 1) \n",
    "        \n",
    "        # if the difference < 0, we put \"0x\" before the absolute value of x\n",
    "        # if the difference >= 0, we put \"1x\" before the absolute value of x\n",
    "        # the concatenation is encoded_raw_diff_str\n",
    "        encoded_raw_diff_str = [\"%s%d\" % (\"1x\" if x < 0 else \"0x\" , abs(x)) for x in encoded_raw_diff]\n",
    "        \n",
    "        # create a dataframe for encoded_raw_diff_str\n",
    "        df = pd.DataFrame(encoded_raw_diff_str)\n",
    "        \n",
    "        # set the column as 'delta'\n",
    "        df.columns = ['delta']\n",
    "        \n",
    "        # count the number of unique encoded_raw_diff_str and sum them up \n",
    "        # and create a corresponding dataframe\n",
    "        df2 = pd.DataFrame(pd.Series(encoded_raw_diff_str).value_counts())\n",
    "        \n",
    "        # set the column as 'total'\n",
    "        df2.columns = ['total']\n",
    "        \n",
    "        # set the 'delta''s column as the index\n",
    "        df2['delta'] = df2.index\n",
    "        \n",
    "        # reset the index starting from 0 \n",
    "        df2 = df2.reset_index(drop=True)\n",
    "        \n",
    "        # set the columns as 'total' and 'delta'\n",
    "        df2.columns = ['total', 'delta']\n",
    "        \n",
    "        # prune all of the total that is < vocabulary_mimimum_word_frequency, in this case it is 10\n",
    "        df2 = df2[df2['total'] < vocabulary_mimimum_word_frequency]\n",
    "        \n",
    "        # df.delta.isin(df2.delta) check if any 'delta' element in df is in the df2.delta, \n",
    "        # if it is in the df2.delta, it would output True. Otherwise, False.\n",
    "        # set the corresponding True column to 'dummy_word = 0xffffffff' in the 'delta' column of df \n",
    "        df.loc[df.delta.isin(df2.delta), ['delta']] = dummy_word\n",
    "        \n",
    "        # we only keep the column 'delta' by removing the indices and store the remaining dataframe to encoded_raw_diff_pruned\n",
    "        encoded_raw_diff_pruned = df['delta']\n",
    "        \n",
    "        # delete df and df2\n",
    "        del df, df2\n",
    "        \n",
    "        # call sklearn.model_selection.train_test_split to split half to training set and other half to test set \n",
    "        tmp_train, tmp_test = train_test_split(encoded_raw_diff_pruned, test_size=0.5, shuffle=False)\n",
    "        \n",
    "        # count how many dummy_word is in encoded_raw_diff_pruned, i.e. how many df.delta is in df2.delta\n",
    "        total_removals = Counter(encoded_raw_diff_pruned)[dummy_word]\n",
    "        \n",
    "        # store the length of encoded_raw_diff_pruned into total_rows\n",
    "        total_rows = len(encoded_raw_diff_pruned)\n",
    "        \n",
    "        # count how many dummy_word is in tmp_train (half of the encoded_raw_diff_pruned)\n",
    "        train_removals = Counter(tmp_train)[dummy_word]\n",
    "        \n",
    "        # store the length of tmp_train into train_total \n",
    "        train_total = len(tmp_train)\n",
    "        \n",
    "        # count how many dummy_word is in tmp_train (half of the encoded_raw_diff_pruned)\n",
    "        # QUESTION: why don't we just get test_removals = total_removals - train_removals?\n",
    "        test_removals = Counter(tmp_test)[dummy_word]\n",
    "        \n",
    "        # store the length of tmp_test into test_total \n",
    "        test_total = len(tmp_test)\n",
    "        \n",
    "        # we define the max_test_accuracy = 1 - test_removals/test_total, i.e.\n",
    "        # 1 - test_removals/test_total: the proportion of NOT dummy_word in testing set\n",
    "        # as dummy_word has different pattern from other 'delta' entries, theoretically all other entries can be predicted correctly \n",
    "        # QUESTION: why is it impossible to predict the dummy_word correctly?\n",
    "        max_test_accuracy = 1-test_removals/test_total\n",
    "        \n",
    "        # we convert each of the 'delta' value in x to string and store them as list in encoded_raw_diff_str\n",
    "        encoded_raw_diff_pruned_str = [str(x) for x in list(encoded_raw_diff_pruned)]\n",
    "        \n",
    "        # build a new tokenizer from keras.preprocessing.text.Tokenizer\n",
    "        tokenizer2 = Tokenizer()\n",
    "        \n",
    "        # fit the dataframe by calling fit_on_texts so that we can assign a uniquely\n",
    "        # identified number to each unique 'delta' values \n",
    "        tokenizer2.fit_on_texts(encoded_raw_diff_pruned_str)\n",
    "        \n",
    "        # As we fit_on_texts in the last step, we would fetch the corresponding \n",
    "        # identification number sequence from the input dataframe \n",
    "        encoded_final = tokenizer2.texts_to_sequences([' '.join(encoded_raw_diff_pruned_str)])[0]\n",
    "        \n",
    "        # vocab_size_raw is the number of unique 'delta' values + 1\n",
    "        final_vocab_size = len(tokenizer2.word_index) + 1\n",
    "    else:\n",
    "        encoded_final = encoded_raw\n",
    "        \n",
    "        final_vocab_size = vocab_size_raw\n",
    "        \n",
    "    # create minibatches of encoded_final of length (look_back + 1), in this case length = 2\n",
    "    # and store sequences of minibatches into variable 'sequences'\n",
    "    sequences = create_windowed_dataset(encoded_final, look_back=10)\n",
    "    \n",
    "    # return the maximum length of minibatch among all minibatches in sequences \n",
    "    max_length = max([len(seq) for seq in sequences])\n",
    "    \n",
    "    # Pads sequences to the same length, \n",
    "    # padding = 'pre' OR 'post': pad either before or after each minibatch\n",
    "    # if we choose minibatch length = 2, we do not have to pad because sequences already contain\n",
    "    # minibatches of the same length = 2\n",
    "    sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
    "    \n",
    "    # X: all columns except the last column\n",
    "    # y: the last column \n",
    "    X, y = sequences[:,:-1], sequences[:,-1]\n",
    "    \n",
    "    # Converts a class vector (integers) to binary class matrix. \n",
    "    # y = to_categorical(y, num_classes=final_vocab_size)\n",
    "    \n",
    "    # call sklearn.model_selection.train_test_split to split 80% to training set and 20% to test set\n",
    "    # parameter random_state=42 takes in a random seed and randomly split into training set and testing set \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "    \n",
    "    # return all the necessary results for building models below\n",
    "    return sequences, final_vocab_size, max_length, X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model_name, final_vocab_size, max_length, X_train, y_train, X_test, y_test):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(final_vocab_size, output_dim=10, input_length=max_length - 1))\n",
    "    embed = K.function([model.layers[0].input], [model.layers[0].output])\n",
    "    embed_train = embed([X_train])[0]\n",
    "    embed_test = embed([X_test])[0]\n",
    "    if model_name == 'lstm':\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(final_vocab_size, output_dim=10, input_length=max_length - 1))\n",
    "        model.add(LSTM(10))\n",
    "        model.add(Dense(final_vocab_size, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        model.fit(X_train, y_train, epochs=20, verbose=True, shuffle=False, batch_size=64)\n",
    "        test_history = model.evaluate(X_test, y_test, batch_size=1000, verbose=True)\n",
    "        return test_history[1]\n",
    "    elif model_name == 'linear':\n",
    "        reg = LinearRegression().fit(embed_train.reshape(embed_train.shape[0], -1), y_train)\n",
    "        y_pred = reg.predict(embed_test.reshape(embed_test.shape[0], -1))\n",
    "        y_pred[y_pred >= 0.5] = 1\n",
    "        y_pred[y_pred < 0.5] = 0\n",
    "        return accuracy_score(y_test, y_pred)\n",
    "    elif model_name == 'nb':\n",
    "        clf = GaussianNB()\n",
    "        clf.fit(embed_train.reshape(embed_train.shape[0], -1), y_train)\n",
    "        y_pred = clf.predict(embed_test.reshape(embed_test.shape[0], -1))\n",
    "        y_pred[y_pred >= 0.5] = 1\n",
    "        y_pred[y_pred < 0.5] = 0\n",
    "        return accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1263124666481274"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences, final_vocab_size, max_length, X_train, X_test, y_train, y_test = dataset_creator('./blackscholes_500k.txt', use_manual_encoding=True)\n",
    "run_model('linear', final_vocab_size, max_length, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(266107, 474)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape\n",
    "\n",
    "# look_back = 1, train/test = 8/2, accuracy = 0.13275613275613277\n",
    "# look_back = 3, train/test = 8/2, accuracy = 0.1328633487155591\n",
    "# look_back = 10, train/test = 8/2, accuracy = 0.1328803775967291"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_names = ['./blackscholes_500k.txt', './fluidanimate_500k.txt', './swaptions_500k.txt']\n",
    "model_names = ['lstm', 'linear', 'nb']\n",
    "for app in app_names:\n",
    "    for model in model_names:\n",
    "        sequences, final_vocab_size, max_length, X_train, X_test, y_train, y_test = dataset_creator(app, use_manual_encoding=True)\n",
    "        run_model(model, final_vocab_size, max_length, X_train, y_train, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
